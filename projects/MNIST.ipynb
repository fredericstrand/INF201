{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network for the MNIST dataset\n",
    "\n",
    "### Importing and inspecting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing from sklearn\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# Inspecting four first images\n",
    "_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n",
    "for ax, image, label in zip(axes, digits.images, digits.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "    ax.set_title(f\"Image: %i\" % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the data\n",
    "\n",
    "Just checking if the data is relatively evenly distrubuted, because this will effect the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the accurances of different values\n",
    "label_counts = np.bincount(digits.target)\n",
    "\n",
    "plt.bar(range(len(label_counts)), label_counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1437, 64), (1437,), (360, 64), (360,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the data and targets\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# Shuffle the data \n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(len(X))\n",
    "\n",
    "# Define the split ratio for the training and test sets\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(X) * split_ratio)\n",
    "\n",
    "# Split the indices into training and testing\n",
    "X_train, y_train = X[indices[:split_index]], y[indices[:split_index]]\n",
    "X_test, y_test = X[indices[split_index:]], y[indices[split_index:]]\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the forward pass for the model\n",
    "\n",
    "First we need to make the forward pass for the neural network. The forward pass consists of:\n",
    "\n",
    "**Weighted sum**\n",
    "\n",
    "$$\n",
    "Z^{(l)} = A^{(l-1)} W^{(l)} + b^{(l)}\n",
    "$$\n",
    "\n",
    "This gives a linear transformation of the input data by combining inputs and weights. The activation function is then applied to introduce non-linearity\n",
    "\n",
    "**Activation functions**\n",
    "\n",
    "* **ReLU activation function**\n",
    "$$\n",
    "f(z) = \\text{ReLU}(z) = \\max(0, z)\n",
    "$$\n",
    "\n",
    "* **Softmax**\n",
    "    Softmax converts raw prediction scores (logits) into probabilities. It is used in the output layer of multi-class classification problems like MNIST. It ensures    that the sum of probabilities across all classes is 1.\n",
    "$$\n",
    "\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return max(0,x)\n",
    "\n",
    "def relu_derivative(x): \n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    y_pred = np.clip(y_pred, 1e-12, 1. - 1e-12)\n",
    "    return -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the backpropegation\n",
    "\n",
    "The **backpropagation** process in a neural network is used to update the weights and biases based on the error (loss) computed during the forward pass. It works by calculating the gradients of the loss function with respect to the weights and propagating the error backward through the network using the chain rule of differentiation.\n",
    "\n",
    "##### Step 1: Categorical Cross-Entropy Loss\n",
    "\n",
    "For the backpropagation process, we first need to compute the error using a loss function. In the case of classification tasks, we typically use **categorical cross-entropy loss**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\hat{y}, y) = - \\sum_{i=1}^{C} y_i \\log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( y_i \\) is the true label (one-hot encoded vector),\n",
    "- \\( \\hat{y}_i \\) is the predicted probability for class \\( i \\),\n",
    "- \\( C \\) is the number of classes.\n",
    "\n",
    "This loss measures how far the predicted probability distribution \\( \\hat{y} \\) is from the true label \\( y \\).\n",
    "\n",
    "\n",
    "##### Step 2: Gradient of the Loss with Respect to Output\n",
    "\n",
    "Once the loss is computed, we calculate the gradient of the loss with respect to the output of the network (i.e., the softmax probabilities \\( \\hat{y} \\)). The gradient for **categorical cross-entropy** is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}_i} = \\hat{y}_i - y_i\n",
    "$$\n",
    "\n",
    "This gradient tells us how much the predicted probability \\( \\hat{y}_i \\) deviates from the true label \\( y_i \\).\n",
    "\n",
    "\n",
    "##### Step 3: Backpropagation to Output Layer\n",
    "\n",
    "For the **output layer**, the backpropagation process begins by computing the gradient of the loss with respect to the weighted sum \\( Z^{(L)} \\) at the output layer. Using the chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial Z^{(L)}} = \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial Z^{(L)}}\n",
    "$$\n",
    "\n",
    "Since \\( \\frac{\\partial \\hat{y}}{\\partial Z^{(L)}} \\) is simplified by the properties of softmax, this results in:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial Z^{(L)}} = \\hat{y} - y\n",
    "$$\n",
    "\n",
    "This is the error signal we propagate back through the network.\n",
    "\n",
    "\n",
    "##### Step 4: Backpropagation to Hidden Layers\n",
    "\n",
    "For each hidden layer \\( l \\), we propagate the error backward using the chain rule. The gradient of the loss with respect to the weights and biases in layer \\( l \\) is calculated as follows:\n",
    "\n",
    "**Gradient of Weights:**\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W^{(l)}} = A^{(l-1)T} \\cdot \\delta^{(l)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( A^{(l-1)} \\) is the activation from the previous layer (or input data for the first layer),\n",
    "- \\( \\delta^{(l)} \\) is the error signal propagated from the next layer.\n",
    "\n",
    "**Gradient of Biases:**\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b^{(l)}} = \\sum \\delta^{(l)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( \\sum \\delta^{(l)} \\) is the sum of the error signal over all the samples in the batch.\n",
    "\n",
    "**Gradient of Input:**\n",
    "$$\n",
    "\\delta^{(l-1)} = \\delta^{(l)} \\cdot W^{(l)T}\n",
    "$$\n",
    "\n",
    "This step propagates the error backward to the previous layer so that its weights and biases can also be updated.\n",
    "\n",
    "\n",
    "##### Step 5: Weight Update Rule\n",
    "\n",
    "Finally, we update the weights and biases using **gradient descent**. The update rule for the weights is:\n",
    "\n",
    "$$\n",
    "W^{(l)} = W^{(l)} - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W^{(l)}}\n",
    "$$\n",
    "\n",
    "And for the biases:\n",
    "\n",
    "$$\n",
    "b^{(l)} = b^{(l)} - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial b^{(l)}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( \\alpha \\) is the learning rate,\n",
    "- \\( \\frac{\\partial \\mathcal{L}}{\\partial W^{(l)}} \\) and \\( \\frac{\\partial \\mathcal{L}}{\\partial b^{(l)}} \\) are the gradients of the loss with respect to the weights and biases, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    y_pred = np.clip(y_pred, 1e-12, 1. - 1e-12)\n",
    "    return -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        # Initialize weights with small random values and biases to 0\n",
    "        self._weights = np.random.randn(input_size, output_size) * 0.1\n",
    "        self._bias = np.zeros((1, output_size))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Calculate the weighted sum of inputs + biases\n",
    "        self._inputs = inputs\n",
    "        self.output = np.dot(inputs, self._weights) + self._bias\n",
    "\n",
    "    def backward(self, d_output, learning_rate):\n",
    "        # Gradient of weights and biases based on the error signal (d_output)\n",
    "        d_weights = np.dot(self._inputs.T, d_output)\n",
    "        d_biases = np.sum(d_output, axis=0, keepdims=True)\n",
    "\n",
    "        # Gradient of the input to pass to the previous layer\n",
    "        d_inputs = np.dot(d_output, self._weights.T)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self._weights -= learning_rate * d_weights\n",
    "        self._biases -= learning_rate * d_biases\n",
    "\n",
    "        return d_inputs\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize two layers: one hidden and one output\n",
    "        self.layer1 = Layer(input_size, hidden_size)\n",
    "        self.layer2 = Layer(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Forward pass through the first layer with ReLU activation\n",
    "        self.layer1.forward(X)\n",
    "        self.layer1_output = relu(self.layer1.output)\n",
    "\n",
    "        # Forward pass through the second layer with Softmax activation\n",
    "        self.layer2.forward(self.layer1_output)\n",
    "        self.output = softmax(self.layer2.output)\n",
    "\n",
    "    def backward(self, X, y, learning_rate):\n",
    "        # Backward pass using Cross-Entropy Loss\n",
    "        loss = cross_entropy_loss(y, self.output)\n",
    "        d_output = self.output - y\n",
    "\n",
    "        # Backward pass through second layer\n",
    "        d_hidden = self.layer2.backward(d_output, learning_rate)\n",
    "\n",
    "        # Backward pass through the first layer with ReLU activation\n",
    "        d_inputs = self.layer1.backward(d_hidden * relu_derivative(self.layer1_output), learning_rate)\n",
    "\n",
    "    def train(self, X_train, y_train, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            self.forward(X_train)\n",
    "            self.backward(X_train, y_train, learning_rate)\n",
    "\n",
    "            # Print the loss every 100 epochs\n",
    "            if epoch % 100 == 0:\n",
    "                loss = cross_entropy_loss(y_train, self.output)\n",
    "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Forward pass for predictions\n",
    "        self.forward(X)\n",
    "        return np.argmax(self.output, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1437,64) and (784,64) not aligned: 64 (dim 1) != 784 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m nn \u001b[38;5;241m=\u001b[39m NeuralNetwork(input_size, hidden_size, output_size)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Train the neural network\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 54\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[0;34m(self, X_train, y_train, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, X_train, y_train, epochs, learning_rate):\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 54\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackward(X_train, y_train, learning_rate)\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;66;03m# Print the loss every 100 epochs\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[32], line 34\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Forward pass through the first layer with ReLU activation\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1_output \u001b[38;5;241m=\u001b[39m relu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1\u001b[38;5;241m.\u001b[39moutput)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# Forward pass through the second layer with Softmax activation\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[32], line 10\u001b[0m, in \u001b[0;36mLayer.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Calculate the weighted sum of inputs + biases\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inputs \u001b[38;5;241m=\u001b[39m inputs\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_weights\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bias\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1437,64) and (784,64) not aligned: 64 (dim 1) != 784 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = 784\n",
    "hidden_size = 64\n",
    "output_size = 10\n",
    "epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Initialize the neural network\n",
    "nn = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# Train the neural network\n",
    "nn.train(X_train, y_train, epochs, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inf201env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
